{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ellinei/229352-StatisticalLearning/blob/main/Lab02_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X33DsAOeokmy"
      },
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
        "\n",
        "# For Fashion-MNIST\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# For 20 Newsgroups\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "metadata": {
        "id": "JOnoRSjo8dd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Marketing Campaign Dataset - Manual Data Preprocessing & Logistic Regression"
      ],
      "metadata": {
        "id": "exuopfRD8mHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Marketing Campaign Dataset ([Data Information](https://archive.ics.uci.edu/dataset/222/bank+marketing))\n",
        "\n",
        "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (`'yes'`) or not (`'no'`) subscribed."
      ],
      "metadata": {
        "id": "FdWsJYcVBf1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank_url = 'https://raw.githubusercontent.com/donlap/ds352-labs/main/bank.csv'\n",
        "\n",
        "df = pd.read_csv(bank_url, sep=';', na_values=['unknown'])\n",
        "df = df.drop([\"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\t\"euribor3m\", \"nr.employed\"], axis=1)\n",
        "print(\"Shape of the dataset:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CJ9ujPUBBcTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration"
      ],
      "metadata": {
        "id": "x4invEyyBRDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Missing Values Count ---\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "u6wANpgzBQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Unique Values for Categorical Columns ---\")\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    print(f\"\\n'{col}' unique values:\")\n",
        "    print(df[col].value_counts(dropna=False)) # Include NaN counts"
      ],
      "metadata": {
        "id": "1ack00iLnuCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "iCEtO_EWDTzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map target variable 'y' to 0 (no) and 1 (yes)\n",
        "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
        "\n",
        "# Drop 'duration' due to data leakage\n",
        "df = df.drop('duration', axis=1)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('y', axis=1)\n",
        "y = df['y']\n",
        "\n",
        "# Split the data BEFORE any transformations\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Print data shape\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "id": "W2J1Prc_BXe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply `StandardScaler()`, `OrdinalEncoder()`, and `OneHotEncoder()` on a few selected columns."
      ],
      "metadata": {
        "id": "yM_gRkNjGR53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Numerical Feature: `age` and `campaign` (Standard Scaling)**"
      ],
      "metadata": {
        "id": "r4sPr5qDGY65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols_demo = ['age', 'campaign']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler ONLY on the training data\n",
        "scaler.fit(X_train[num_cols_demo])\n",
        "\n",
        "X_train_scaled_demo = scaler.transform(X_train[num_cols_demo])\n",
        "X_test_scaled_demo = scaler.transform(X_test[num_cols_demo])"
      ],
      "metadata": {
        "id": "52ibTHHFDxSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the transformed `age` and `campaign` features and their statistics."
      ],
      "metadata": {
        "id": "YnT6pEq4LlZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal X_train 'age' and 'campaign' head:\")\n",
        "print(X_train[num_cols_demo].head())\n",
        "print(\"\\nScaled X_train 'age' and 'campaign' head:\")\n",
        "print(pd.DataFrame(X_train_scaled_demo, columns=num_cols_demo, index=X_train.index).head())\n",
        "\n",
        "print(\"\\nMean of scaled 'age' (train):\", X_train_scaled_demo[:, 0].mean())\n",
        "print(\"Std Dev of scaled 'campaign' (train):\", X_train_scaled_demo[:, 1].std())"
      ],
      "metadata": {
        "id": "7bfFm4InLiGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Ordinal Feature: `education` (Ordinal Encoding with Imputation)**"
      ],
      "metadata": {
        "id": "8jQi5cJAGj_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Imputation**"
      ],
      "metadata": {
        "id": "2uyk1rA7K7_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ord_col_demo = ['education']\n",
        "\n",
        "imputer_ord = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Fit the imputer ONLY on the training data and transform both train and test data\n",
        "imputer_ord.fit(X_train[ord_col_demo])\n",
        "\n",
        "X_train_imputed_ord_demo = imputer_ord.transform(X_train[ord_col_demo])\n",
        "X_test_imputed_ord_demo = imputer_ord.transform(X_test[ord_col_demo])"
      ],
      "metadata": {
        "id": "A8RbFYHLIhju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Ordinal Encoding**"
      ],
      "metadata": {
        "id": "P9hDuDPyLGaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "education_categories = [\n",
        "    'illiterate', 'basic.4y', 'basic.6y', 'basic.9y', 'high.school',\n",
        "    'professional.course', 'university.degree', 'masters', 'doctorate'\n",
        "]"
      ],
      "metadata": {
        "id": "n7DMoQ6AGf8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_encoder = OrdinalEncoder(categories=[education_categories])\n",
        "\n",
        "X_train_ord_encoded_demo = ordinal_encoder.fit_transform(X_train_imputed_ord_demo)\n",
        "X_test_ord_encoded_demo = ordinal_encoder.transform(X_test_imputed_ord_demo)"
      ],
      "metadata": {
        "id": "MjbM7wi8IfHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the imputed and ordinal-encoded `education`."
      ],
      "metadata": {
        "id": "tmYa8EfcOCcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal X_train 'education' head:\")\n",
        "print(X_train[ord_col_demo].iloc[20:25])\n",
        "print(\"\\nImputed X_train 'education' head (after imputer.transform):\")\n",
        "print(pd.DataFrame(X_train_imputed_ord_demo, columns=ord_col_demo, index=X_train.index).iloc[20:25])\n",
        "print(\"\\nOrdinal Encoded X_train 'education' head:\")\n",
        "print(pd.DataFrame(X_train_ord_encoded_demo, columns=ord_col_demo, index=X_train.index).iloc[20:25])"
      ],
      "metadata": {
        "id": "JrJoFy3-Ik1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Nominal Feature: `job` (One-Hot Encoding with Imputation)**"
      ],
      "metadata": {
        "id": "GarEnu8iK1lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Imputation**"
      ],
      "metadata": {
        "id": "1eJSiEslLNi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nom_col_demo = ['job']\n",
        "\n",
        "imputer_nom = SimpleImputer(strategy='most_frequent')\n",
        "imputer_nom.fit(X_train[nom_col_demo])\n",
        "\n",
        "X_train_imputed_nom_demo = imputer_nom.transform(X_train[nom_col_demo])\n",
        "X_test_imputed_nom_demo = imputer_nom.transform(X_test[nom_col_demo])"
      ],
      "metadata": {
        "id": "1OUbef1bJPUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Nominal Encoding**"
      ],
      "metadata": {
        "id": "l7F_kim6TQ72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "# Fit on training data and transform both train and test data\n",
        "X_train_onehot_encoded_demo = onehot_encoder.fit_transform(X_train_imputed_nom_demo)\n",
        "X_test_onehot_encoded_demo = onehot_encoder.transform(X_test_imputed_nom_demo)"
      ],
      "metadata": {
        "id": "6kMCGjksLyrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal X_train 'job' head:\")\n",
        "print(X_train[nom_col_demo].iloc[40:45])\n",
        "print(\"\\nImputed X_train 'job' head (after imputer.transform):\")\n",
        "print(pd.DataFrame(X_train_imputed_nom_demo, columns=nom_col_demo, index=X_train.index).iloc[40:45])\n",
        "print(\"\\nOne-Hot Encoded X_train 'job' shape:\", X_train_onehot_encoded_demo.shape)\n",
        "print(\"First 5 rows of One-Hot Encoded X_train 'job':\")\n",
        "print(pd.DataFrame(X_train_onehot_encoded_demo, columns=onehot_encoder.get_feature_names_out(nom_col_demo), index=X_train.index).iloc[40:45])"
      ],
      "metadata": {
        "id": "j1qIKIelM0yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Apply All Preprocessing & Train Logistic Regression**\n",
        "\n",
        "Now, it's your turn to apply these preprocessing steps to *all* relevant columns and then train a Logistic Regression model.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  Look at the Variable Table in [this link](https://archive.ics.uci.edu/dataset/222/bank+marketing).\n",
        "2. Make lists for `numerical_features`, `ordinal_features`, and `nominal_features`.\n",
        "3. Preprocess the features. It is safer to make a copy of `X_train` using:\n",
        "   ```\n",
        "   X_train_copy = X_train.copy()\n",
        "   X_test_copy = X_test.copy()\n",
        "   ```\n",
        "   and preprocess `X_train_copy` instead.\n",
        "\n",
        "   **For nominal features, concat the one-hot encoded features using [`pd.concat(..., axis=1)`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) and drop the old nominal features from the dataframe.**\n",
        "4. Train Logistic Regression on the preprocessed `X_train_copy` and `y_train`.\n",
        "5. Evaluate the Model:\n",
        "    *   Make predictions on the preprocessed `X_test_copy`.\n",
        "    *   Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?\n"
      ],
      "metadata": {
        "id": "CllpFvNBNYWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 1 STARTS HERE ---\n",
        "\n",
        "numerical_features = ['age', 'campaign', 'previous', 'pdays']\n",
        "ordinal_features = ['education']\n",
        "nominal_features = ['job', 'marital', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'default']\n",
        "\n",
        "X_train_copy = X_train.copy()\n",
        "X_test_copy = X_test.copy()"
      ],
      "metadata": {
        "id": "jPEL9Bz2lQ2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_copy[numerical_features] = scaler.fit_transform(X_train_copy[numerical_features])\n",
        "X_test_copy[numerical_features] = scaler.transform(X_test_copy[numerical_features])"
      ],
      "metadata": {
        "id": "MT75-7JolQvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer_ord = SimpleImputer(strategy='most_frequent')\n",
        "X_train_copy[ordinal_features] = imputer_ord.fit_transform(X_train_copy[ordinal_features])\n",
        "X_test_copy[ordinal_features] = imputer_ord.transform(X_test_copy[ordinal_features])\n",
        "\n",
        "education_categories = [\n",
        "    'illiterate', 'basic.4y', 'basic.6y', 'basic.9y', 'high.school',\n",
        "    'professional.course', 'university.degree', 'masters', 'doctorate'\n",
        "]\n",
        "ordinal_encoder = OrdinalEncoder(categories=[education_categories])\n",
        "X_train_copy[ordinal_features] = ordinal_encoder.fit_transform(X_train_copy[ordinal_features])\n",
        "X_test_copy[ordinal_features] = ordinal_encoder.transform(X_test_copy[ordinal_features])"
      ],
      "metadata": {
        "id": "Wl8_Kx3zlQpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer_nom = SimpleImputer(strategy='most_frequent')\n",
        "X_train_copy[nominal_features] = imputer_nom.fit_transform(X_train_copy[nominal_features])\n",
        "X_test_copy[nominal_features] = imputer_nom.transform(X_test_copy[nominal_features])\n",
        "\n",
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "X_train_onehot = onehot_encoder.fit_transform(X_train_copy[nominal_features])\n",
        "X_test_onehot = onehot_encoder.transform(X_test_copy[nominal_features])\n",
        "\n",
        "X_train_onehot_df = pd.DataFrame(X_train_onehot, columns=onehot_encoder.get_feature_names_out(nominal_features), index=X_train_copy.index)\n",
        "X_test_onehot_df = pd.DataFrame(X_test_onehot, columns=onehot_encoder.get_feature_names_out(nominal_features), index=X_test_copy.index)\n",
        "\n",
        "X_train_copy = X_train_copy.drop(nominal_features, axis=1)\n",
        "X_test_copy = X_test_copy.drop(nominal_features, axis=1)\n",
        "\n",
        "X_train_processed = pd.concat([X_train_copy, X_train_onehot_df], axis=1)\n",
        "X_test_processed = pd.concat([X_test_copy, X_test_onehot_df], axis=1)"
      ],
      "metadata": {
        "id": "do-wivfXlQkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "model.fit(X_train_processed, y_train)"
      ],
      "metadata": {
        "id": "if22feh_lQeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_processed)"
      ],
      "metadata": {
        "id": "4NJkafXKlQXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"Accuracy Score\")\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "xuI8hAlIRfDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Fashion-MNIST Dataset - Image Classification"
      ],
      "metadata": {
        "id": "m9qrm2DKRtgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Fashion-MNIST Dataset\n",
        "\n",
        "The Fashion-MNIST dataset consists of 28x28 grayscale images of fashion items."
      ],
      "metadata": {
        "id": "kc8SZBvcS8_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(fm_X_train, fm_y_train), (fm_X_test, fm_y_test) = fashion_mnist.load_data()\n",
        "\n",
        "print(f\"Fashion-MNIST Train data shape: {fm_X_train.shape}\")\n",
        "print(f\"Fashion-MNIST Train labels shape: {fm_y_train.shape}\")\n",
        "print(f\"Fashion-MNIST Test data shape: {fm_X_test.shape}\")\n",
        "print(f\"Fashion-MNIST Test labels shape: {fm_y_test.shape}\")"
      ],
      "metadata": {
        "id": "r0FQt8rlRgoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First image {fm_X_train[0]}\")\n",
        "print(f\"First label {fm_y_train[0]}\")"
      ],
      "metadata": {
        "id": "LXJ9EmcIVVrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Fashion-MNIST Images\n",
        "\n",
        "Let's see what these images look like."
      ],
      "metadata": {
        "id": "WwkQOE79TV70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist_class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "# Visualize the images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(fm_X_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(fashion_mnist_class_names[fm_y_train[i]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-YUI6IzbTGYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Preprocessing Images (Flatten and Scale)**\n",
        "\n",
        "Images are 2D arrays (matrices of pixels) and pixel values are integers from 0-255. For Logistic Regression, we need:\n",
        "*  **Flattening:** Convert each 28x28 image into a 1D array of 784 features.\n",
        "*  **Scaling:** Normalize pixel values from [0, 255] to [0, 1].\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.   **Flatten:** Use the `.reshape()` method (see [documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html)). For `fm_X_train_binary` (shape `(num_samples, 28, 28)`), you want to reshape it to `(num_samples, 28*28)`.\n",
        "2.  **Scale:** Divide the flattened pixel values by 255.0 to get values between 0 and 1.\n",
        "3.   **Train Logistic Regression:**\n",
        "    *   Initialize `LogisticRegression(solver='saga')`. `saga` is a good solver when both number of samples and number of features are large.\n",
        "    *   Fit the model on your *processed* `fm_X_train_scaled` and `fm_y_train`.\n",
        "4.   **Make Predictions:** Use `predict()` to make predictions on the *processed* `fm_X_test_scaled`.\n",
        "5.   **Print Classification Report:** Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?\n",
        "6.   **Visualize Misclassifications:**\n",
        "    *   Find the indices in `fm_X_test_binary` where your model made incorrect predictions (i.e., `fm_y_pred != fm_y_test`).\n",
        "    *   Select 5 of these misclassified images.\n",
        "    *   Plot these images (using `plt.imshow`). For each image, print its true label and its predicted label."
      ],
      "metadata": {
        "id": "cXvJB42xVrHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 2 STARTS HERE ---\n",
        "\n",
        "fm_X_train_scaled = fm_X_train.reshape(fm_X_train.shape[0], -1)\n",
        "fm_X_test_scaled = fm_X_test.reshape(fm_X_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "PwYMXRglk9_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fm_X_train_scaled = fm_X_train_scaled / 255.0\n",
        "fm_X_test_scaled = fm_X_test_scaled / 255.0\n",
        "\n",
        "print(f\"Flattened and scaled train data shape: {fm_X_train_scaled.shape}\")\n",
        "print(f\"Flattened and scaled test data shape: {fm_X_test_scaled.shape}\")"
      ],
      "metadata": {
        "id": "nbnjCisAk93r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model = LogisticRegression(solver='saga', random_state=42, max_iter=1000)\n",
        "logistic_regression_model.fit(fm_X_train_scaled, fm_y_train)"
      ],
      "metadata": {
        "id": "Y4F-BC7Ik9r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fm_y_pred = logistic_regression_model.predict(fm_X_test_scaled)"
      ],
      "metadata": {
        "id": "8qRP7e4kk9UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(fm_y_test, fm_y_pred))"
      ],
      "metadata": {
        "id": "kaLnR8oUk8-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_indices = np.where(fm_y_pred != fm_y_test)[0]\n",
        "num_misclassified_to_show = 5\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, index in enumerate(misclassified_indices[:num_misclassified_to_show]):\n",
        "    plt.subplot(1, num_misclassified_to_show, i + 1)\n",
        "    plt.imshow(fm_X_test[index], cmap='gray')\n",
        "    plt.title(f\"True: {fashion_mnist_class_names[fm_y_test[index]]}\\nPred: {fashion_mnist_class_names[fm_y_pred[index]]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P_Z7ooAgdLVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: 20 Newsgroups Dataset - Text Classification"
      ],
      "metadata": {
        "id": "u7-q5FmnboVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load 20 Newsgroups Dataset\n",
        "\n",
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics."
      ],
      "metadata": {
        "id": "6TDncAyyb6mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "news_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
        "\n",
        "X_train_news, y_train_news = news_train.data, news_train.target\n",
        "X_test_news, y_test_news = news_test.data, news_test.target\n",
        "\n",
        "print(f\"Number of training documents: {len(X_train_news)}\")\n",
        "print(f\"Number of test documents: {len(X_test_news)}\")\n",
        "print(f\"Categories: {news_train.target_names}\")"
      ],
      "metadata": {
        "id": "E91xZl5NbnpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Sample Document"
      ],
      "metadata": {
        "id": "tVCq09V2cfGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first document and its class\n",
        "## Write your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "G37RNZGebFZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing: Text Vectorization Demonstration with `TfidfVectorizer`"
      ],
      "metadata": {
        "id": "1vXXUdp7chsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{\\text{number of word }t\\text{ in } d}{\\text{number of words in } d} \\quad \\text{ and } \\quad\n",
        "\\text{IDF}(t, D) = \\log\\left(\\frac{\\text{total number of documents}}{\\text{number of documents that contain word }t}\\right).\n",
        "$$"
      ],
      "metadata": {
        "id": "rwaq1igbi-Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentences = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the sample sentences\n",
        "sample_vec_output_sparse = # Write your code here\n",
        "\n",
        "sample_vec_output_dense = sample_vec_output_sparse.toarray()\n",
        "\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(sample_vec_output_dense)"
      ],
      "metadata": {
        "id": "8AaVd2D9ckKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Apply TF-IDF Vectorization to Full Dataset**\n",
        "\n",
        "Now, apply `TfidfVectorizer` to the actual training and testing datasets for the 20 Newsgroups classification task.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Initialize `TfidfVectorizer`:**\n",
        "    *   Initialize `TfidfVectorizer`. Use `stop_words='english'` to remove common words.\n",
        "2.  **Fit and Transform Training Data:**\n",
        "    *   Call `fit_transform()` on `X_train_news` to learn the vocabulary and transform the training text into TF-IDF features. Store the result in `X_train_vec`.\n",
        "3.  **Transform Test Data:**\n",
        "    *   Call `transform()` on `X_test_news` using the *already fitted* vectorizer. Store the result in `X_test_vec`. **Crucially, do not call `fit_transform()` on the test data!** This would cause data leakage.\n",
        "4.  **Initialize Logistic Regression:**\n",
        "    *   Initialize `LogisticRegression(solver='saga')`. `saga` is a good solver when both number of samples and number of features are large.\n",
        "5.  **Train the Model:**\n",
        "    *   Fit the model on your `X_train_vec` and `y_train_news`.\n",
        "6.  **Make Predictions:**\n",
        "    *   Make predictions using `predict()` on the `X_test_vec`.\n",
        "7.  **Evaluate the Model:**\n",
        "    *   Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?"
      ],
      "metadata": {
        "id": "aCa_dcEDc-PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 3 STARTS HERE ---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B6g7y5yXrczq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}